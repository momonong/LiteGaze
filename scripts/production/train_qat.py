import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms, models
# ä½¿ç”¨é‡åŒ–ç‰ˆæ¨¡å‹åº«
from torchvision.models.quantization import mobilenet_v3_large
import os
import glob
from PIL import Image
from tqdm import tqdm
import numpy as np

# ================= âš™ï¸ QAT è¨­å®š =================
DATA_DIR = 'data/selfmade_combined'
TEACHER_PATH = 'models/L2CSNet_gaze360.pkl' # ä¾ç„¶éœ€è¦è€å¸«ä¾†æŒ‡å°
PRETRAINED_STUDENT = 'models/student_mobilenet_3people_9k.pth'
QAT_SAVE_PATH = 'models/student_mobilenet_qat.pth'

DEVICE = torch.device('cuda') # QAT è¨“ç·´å¯ä»¥ç”¨ GPU åŠ é€Ÿ
BATCH_SIZE = 64
LR = 1e-5             # ğŸ”¥ éå¸¸å°çš„å­¸ç¿’ç‡ï¼Œåªæ˜¯å¾®èª¿
EPOCHS = 5            # ä¸ç”¨å¤šï¼Œå¹¾è¼ªå°±å¤ é©æ‡‰äº†
# ===============================================

# 1. å®šç¾© QAT æ¨¡å‹çµæ§‹
class L2CS_MobileNetV3_QAT(nn.Module):
    def __init__(self, num_bins=90):
        super(L2CS_MobileNetV3_QAT, self).__init__()
        # quantize=False: å…ˆè¼‰å…¥ FP32 æ¬Šé‡
        self.backbone = mobilenet_v3_large(weights=None, quantize=False)
        in_features = self.backbone.classifier[3].in_features
        self.backbone.classifier[3] = nn.Linear(in_features, num_bins * 2)
        
        # QAT éœ€è¦ Stub
        self.quant = torch.ao.quantization.QuantStub()
        self.dequant = torch.ao.quantization.DeQuantStub()

    def forward(self, x):
        x = self.quant(x)
        x = self.backbone(x)
        x = self.dequant(x)
        return x[:, :90], x[:, 90:]

# 2. è€å¸«æ¨¡å‹ (å›ºå®š)
class L2CS_ResNet50(nn.Module):
    def __init__(self, num_bins=90):
        super(L2CS_ResNet50, self).__init__()
        self.model = models.resnet50(weights=None)
        self.model.fc = nn.Linear(2048, num_bins * 2)
    def forward(self, x):
        x = self.model(x)
        return x[:, :90], x[:, 90:]

# 3. è³‡æ–™é›† (ç°¡å–®ç‰ˆ)
class GazeDataset(torch.utils.data.Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_files = glob.glob(os.path.join(root_dir, "*.jpg"))
    def __len__(self):
        return len(self.image_files)
    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        image = Image.open(img_name).convert('RGB')
        if self.transform:
            image = self.transform(image)
        return image

def main():
    print("ğŸš€ å•Ÿå‹• QAT (é‡åŒ–æ„ŸçŸ¥è¨“ç·´)...")

    # A. æº–å‚™è³‡æ–™
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    dataset = GazeDataset(DATA_DIR, transform=transform)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)

    # B. æº–å‚™è€å¸« (Freeze)
    print("ğŸ“ è¼‰å…¥è€å¸«æ¨¡å‹...")
    teacher = L2CS_ResNet50().to(DEVICE)
    ckpt = torch.load(TEACHER_PATH, map_location=DEVICE)
    # (æ¬Šé‡è¼‰å…¥é‚è¼¯çœç•¥ï¼Œå‡è¨­ä½ ä¹‹å‰ä»£ç¢¼æœ‰ï¼Œé€™é‚Šç°¡åŒ–)
    state = {}
    for k, v in ckpt.items():
        if 'fc_pitch' in k or 'fc_yaw' in k: continue
        nk = 'model.'+k if not k.startswith('model.') else k
        state[nk] = v
    if 'fc_pitch_gaze.weight' in ckpt:
        state['model.fc.weight'] = torch.cat((ckpt['fc_pitch_gaze.weight'], ckpt['fc_yaw_gaze.weight']), 0)
        state['model.fc.bias'] = torch.cat((ckpt['fc_pitch_gaze.bias'], ckpt['fc_yaw_gaze.bias']), 0)
    teacher.load_state_dict(state, strict=False)
    teacher.eval()

    # C. æº–å‚™å­¸ç”Ÿ (QAT Setup)
    print("ğŸ‘¶ è¼‰å…¥ä¸¦æº–å‚™ QAT å­¸ç”Ÿæ¨¡å‹...")
    student = L2CS_MobileNetV3_QAT().to(DEVICE)
    
    # C1. è¼‰å…¥ä¹‹å‰çš„è¨“ç·´æˆæœ (FP32)
    # æ³¨æ„ï¼šå› ç‚ºçµæ§‹ç•¥æœ‰ä¸åŒ (Quantizable Backbone)ï¼Œä½¿ç”¨ strict=False
    saved_state = torch.load(PRETRAINED_STUDENT, map_location=DEVICE)
    student.load_state_dict(saved_state, strict=False)
    
    # C2. è¨­å®š QAT é…ç½®
    student.train()
    # ä½¿ç”¨èˆ‡ Static Quantize ç›¸åŒçš„å¾Œç«¯
    student.qconfig = torch.ao.quantization.get_default_qat_qconfig('qnnpack')
    
    # C3. èåˆç®—å­ (Fusion) - é€™æ˜¯é—œéµï¼
    # é€™æœƒæŠŠ Conv+BN+ReLU åˆä½µæˆä¸€å€‹å±¤ï¼Œè®“é‡åŒ–æ›´æº–
    student.backbone.fuse_model(is_qat=True)
    
    # C4. æº–å‚™ (Prepare QAT) - æ’å…¥ FakeQuant ç¯€é»
    torch.ao.quantization.prepare_qat(student, inplace=True)
    
    # D. é–‹å§‹å¾®èª¿è¨“ç·´
    optimizer = optim.Adam(student.parameters(), lr=LR)
    kl_loss = nn.KLDivLoss(reduction='batchmean')
    T = 5.0 # Temperature

    student = student.to(DEVICE) # ç¢ºä¿åœ¨ GPU ä¸Šè¨“ç·´

    print(f"ğŸ”¥ é–‹å§‹ {EPOCHS} Epochs çš„å¾®èª¿...")
    for epoch in range(EPOCHS):
        total_loss = 0
        student.train() # QAT å¿…é ˆåœ¨ train æ¨¡å¼ä¸‹æ›´æ–° FakeQuant åƒæ•¸
        
        loop = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        for images in loop:
            images = images.to(DEVICE)
            
            # è€å¸«é æ¸¬
            with torch.no_grad():
                t_pitch, t_yaw = teacher(images)
            
            # å­¸ç”Ÿé æ¸¬ (å¸¶æœ‰é‡åŒ–å™ªè²)
            s_pitch, s_yaw = student(images)
            
            # è’¸é¤¾ Loss
            loss_pitch = kl_loss(torch.log_softmax(s_pitch/T, dim=1), torch.softmax(t_pitch/T, dim=1)) * (T**2)
            loss_yaw = kl_loss(torch.log_softmax(s_yaw/T, dim=1), torch.softmax(t_yaw/T, dim=1)) * (T**2)
            loss = loss_pitch + loss_yaw
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            loop.set_postfix(loss=loss.item())
            
        print(f"Epoch {epoch+1} Avg Loss: {total_loss/len(dataloader):.4f}")

    # E. è½‰æ›ç‚ºçœŸæ­£çš„ INT8 æ¨¡å‹ (Convert)
    print("ğŸ”„ æ­£åœ¨è½‰æ›ç‚º INT8 æ¨¡å‹ (CPU)...")
    student.eval()
    student.to('cpu') # è½‰æ›å¿…é ˆåœ¨ CPU
    torch.ao.quantization.convert(student, inplace=True)
    
    torch.save(student.state_dict(), QAT_SAVE_PATH)
    print(f"âœ… QAT æ¨¡å‹å·²å„²å­˜: {QAT_SAVE_PATH}")
    print(f"ğŸ“Š æ¨¡å‹å¤§å°: {os.path.getsize(QAT_SAVE_PATH)/1024**2:.2f} MB")

if __name__ == "__main__":
    main()