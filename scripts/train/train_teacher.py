import os
import glob
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, applications
import random

# === [ğŸ”¥ 5090 ä¿®æ­£: é–‹å•Ÿ GPU è¨˜æ†¶é«”å¢é•·] ===
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… 5090 è¨˜æ†¶é«”å‹•æ…‹å¢é•·å·²é–‹å•Ÿ")
    except RuntimeError as e:
        print(e)

# === [è¨­å®šå€] ===
PROCESSED_DIR = './data/processed'
BATCH_SIZE = 32
EPOCHS = 20
LEARNING_RATE = 1e-4
INPUT_SHAPE = (224, 224, 3)

# === [1. æ–°ç‰ˆè³‡æ–™ç®¡ç·š: ä½¿ç”¨ tf.data] ===
def gaze_data_generator():
    """
    é€™æ˜¯ä¸€å€‹ Python Generatorï¼Œè² è²¬å¾ç¡¬ç¢Ÿéš¨æ©Ÿè®€å– npz
    """
    # æœå°‹æ‰€æœ‰æª”æ¡ˆ
    files = glob.glob(os.path.join(PROCESSED_DIR, '*', '*.npz'))
    if not files:
        raise ValueError("æ‰¾ä¸åˆ°è³‡æ–™ï¼è«‹ç¢ºèª preprocess.py æ˜¯å¦åŸ·è¡Œå®Œæˆã€‚")
        
    random.shuffle(files) # æ¯å€‹ epoch é–‹å§‹å‰æ´—ç‰Œ
    
    for file_path in files:
        try:
            with np.load(file_path) as data:
                # è®€å– Teacher åœ–ç‰‡ (uint8) å’Œ Label
                images = data['teacher'] 
                labels = data['label']
            
            # é€™è£¡æˆ‘å€‘ä¸€æ¬¡ yield ä¸€å¼µåœ–ï¼Œè®“ tf.data å»è² è²¬çµ„è£ batch
            # é€™æ¨£æ›´éˆæ´»ï¼Œä¸”èƒ½åˆ©ç”¨ tf.data çš„ä¸¦è¡Œå„ªå‹¢
            for i in range(len(images)):
                yield images[i], labels[i]
                
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            continue

def create_dataset():
    """
    å°‡ Python Generator è½‰æ›ç‚ºé«˜æ•ˆèƒ½çš„ tf.data.Dataset
    """
    # å®šç¾©è¼¸å‡ºçš„è³‡æ–™æ ¼å¼ (åœ–ç‰‡ 224x224x3, æ¨™ç±¤ 2)
    output_signature = (
        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.uint8),
        tf.TensorSpec(shape=(2,), dtype=tf.float32)
    )

    # 1. å»ºç«‹ Dataset
    dataset = tf.data.Dataset.from_generator(
        gaze_data_generator,
        output_signature=output_signature
    )

    # 2. è³‡æ–™å¢å¼·èˆ‡é è™•ç† (é€™è£¡å¯ä»¥é–‹å¤šæ ¸å¿ƒä¸¦è¡Œ!)
    def preprocess(img, label):
        # è½‰ float ä¸¦æ­¸ä¸€åŒ– (0~1)
        img = tf.cast(img, tf.float32) / 255.0
        # ç¢ºä¿å½¢ç‹€æ­£ç¢º
        img = tf.ensure_shape(img, INPUT_SHAPE)
        return img, label

    # === ğŸš€ æ•ˆèƒ½å…¨é–‹é—œéµ ===
    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE) # å¤šæ ¸å¿ƒåŒæ™‚è™•ç†
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE) # GPU ç®—åœ–æ™‚ï¼ŒCPU é å…ˆè®€ä¸‹ä¸€æ‰¹
    
    return dataset

# === [2. æ¨¡å‹å®šç¾©] ===
def build_teacher_model():
    print("æ­£åœ¨å»ºç«‹ ResNet50V2 æ¨¡å‹...")
    base_model = applications.ResNet50V2(
        include_top=False,
        weights='imagenet',
        input_shape=INPUT_SHAPE
    )
    base_model.trainable = True 

    x = base_model.output
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(2, name='gaze_output')(x)
    
    model = models.Model(inputs=base_model.input, outputs=outputs, name='Teacher_ResNet50')
    return model

# === [3. ä¸»ç¨‹å¼] ===
if __name__ == "__main__":
    # å»ºç«‹ Dataset
    print("ğŸš€ æ­£åœ¨å»ºæ§‹ tf.data é«˜é€Ÿç®¡ç·š...")
    train_ds = create_dataset()
    
    # ç°¡å–®ç®—ä¸€ä¸‹æ­¥æ•¸ (ç‚ºäº†é¡¯ç¤ºé€²åº¦æ¢)
    # ä¼°è¨ˆï¼šæª”æ¡ˆæ•¸ * 1000å¼µ / 32
    num_files = len(glob.glob(os.path.join(PROCESSED_DIR, '*', '*.npz')))
    steps_per_epoch = (num_files * 500) // BATCH_SIZE # ä¿å®ˆä¼°è¨ˆæ¯æª”500å¼µ
    print(f"é ä¼°æ¯å€‹ Epoch éœ€è¦è·‘ {steps_per_epoch} æ­¥")

    # å»ºç«‹æ¨¡å‹
    model = build_teacher_model()
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='mse',
        metrics=['mae']
    )
    
    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(
        filepath='models/teacher_resnet50_best.keras',
        save_best_only=True,
        monitor='loss',
        mode='min'
    )

    # é–‹å§‹è¨“ç·´
    # æ³¨æ„ï¼šé€™è£¡ä¸å†éœ€è¦ workers åƒæ•¸ï¼Œå› ç‚º tf.data è‡ªå‹•æå®šäº†
    history = model.fit(
        train_ds,
        epochs=EPOCHS,
        steps_per_epoch=steps_per_epoch,
        callbacks=[checkpoint_cb]
    )
    
    print("è¨“ç·´å®Œæˆï¼")