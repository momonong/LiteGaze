import os
import glob
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, losses
import random
from student_model import build_student_model # åŒ¯å…¥æˆ‘å€‘å®šç¾©å¥½çš„å­¸ç”Ÿæ¨¡å‹

# === [ğŸ”¥ 5090 è¨˜æ†¶é«”è¨­å®š] ===
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# === [è¨­å®šå€] ===
PROCESSED_DIR = './data/processed'
TEACHER_MODEL_PATH = 'models/teacher_resnet50_best.keras' # å‡è¨­çµ„å“¡å‚³çµ¦ä½ çš„æª”å
BATCH_SIZE = 64 # å­¸ç”Ÿæ¨¡å‹æ¯”è¼ƒå°ï¼ŒBatch å¯ä»¥é–‹å¤§ä¸€é»
EPOCHS = 30
LEARNING_RATE = 1e-3

# è’¸é¤¾æ¬Šé‡ (é—œéµåƒæ•¸)
ALPHA = 0.5  # 0.5 è¡¨ç¤ºï¼šä¸€åŠè½è€å¸«çš„ï¼Œä¸€åŠçœ‹æ¨™æº–ç­”æ¡ˆ

# === [1. é›™è¼¸å…¥è³‡æ–™ç®¡ç·š] ===
def distillation_generator():
    files = glob.glob(os.path.join(PROCESSED_DIR, '*', '*.npz'))
    random.shuffle(files)
    
    for file_path in files:
        try:
            with np.load(file_path) as data:
                img_t = data['teacher'] # 224x224
                img_s = data['student'] # 60x60
                labels = data['label']
            
            # åŒæ™‚åå‡º (Teacheråœ–, Studentåœ–, æ¨™ç±¤)
            for i in range(len(img_t)):
                yield (img_t[i], img_s[i]), labels[i]
                
        except Exception:
            continue

def create_distillation_dataset():
    # å®šç¾©æ ¼å¼: ((T_img, S_img), Label)
    output_signature = (
        (
            tf.TensorSpec(shape=(224, 224, 3), dtype=tf.uint8),
            tf.TensorSpec(shape=(60, 60, 3), dtype=tf.uint8)
        ),
        tf.TensorSpec(shape=(2,), dtype=tf.float32)
    )

    dataset = tf.data.Dataset.from_generator(
        distillation_generator,
        output_signature=output_signature
    )

    def preprocess(inputs, label):
        t_img, s_img = inputs
        # æ­¸ä¸€åŒ–
        t_img = tf.cast(t_img, tf.float32) / 255.0
        s_img = tf.cast(s_img, tf.float32) / 255.0
        
        # å›å‚³å­—å…¸æ ¼å¼ï¼Œè®“ Model æ¯”è¼ƒå¥½è®€
        return {"teacher_input": t_img, "student_input": s_img}, label

    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset

# === [2. å®šç¾©è’¸é¤¾æ¨¡å‹ (Distiller Class)] ===
class Distiller(tf.keras.Model):
    def __init__(self, student, teacher):
        super().__init__()
        self.teacher = teacher
        self.student = student

    def compile(self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha=0.1):
        super().compile(optimizer=optimizer, metrics=metrics)
        self.student_loss_fn = student_loss_fn
        self.distillation_loss_fn = distillation_loss_fn
        self.alpha = alpha

    def train_step(self, data):
        # Unpack data
        x, y_true = data # x æ˜¯ä¸€å€‹å­—å…¸ {'teacher_input': ..., 'student_input': ...}
        
        # Teacher åªåšé æ¸¬ï¼Œä¸è¨“ç·´ (Forward pass only)
        # æ³¨æ„ï¼šTeacher åœ¨è¨“ç·´æ¨¡å¼ä¸‹é€šå¸¸æœƒé—œé–‰ Dropoutï¼Œé€™è£¡æˆ‘å€‘è¨­ training=False
        teacher_predictions = self.teacher(x['teacher_input'], training=False)

        with tf.GradientTape() as tape:
            # Student é€²è¡Œé æ¸¬
            student_predictions = self.student(x['student_input'], training=True)

            # è¨ˆç®—å…©ç¨® Loss
            # 1. Student vs Ground Truth (æ¨™æº–ç­”æ¡ˆ)
            loss_student = self.student_loss_fn(y_true, student_predictions)
            
            # 2. Student vs Teacher (è€å¸«çš„æŒ‡å°)
            loss_distillation = self.distillation_loss_fn(teacher_predictions, student_predictions)

            # 3. ç¸½ Loss (åŠ æ¬Šå¹³å‡)
            total_loss = self.alpha * loss_student + (1 - self.alpha) * loss_distillation

        # è¨ˆç®—æ¢¯åº¦ä¸¦æ›´æ–° Student çš„æ¬Šé‡
        trainable_vars = self.student.trainable_variables
        gradients = tape.gradient(total_loss, trainable_vars)
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        # æ›´æ–°ç›£æ§æŒ‡æ¨™
        self.compiled_metrics.update_state(y_true, student_predictions)
        
        # å›å‚³ç•¶ä¸‹çš„ Loss çµ¦é€²åº¦æ¢é¡¯ç¤º
        results = {m.name: m.result() for m in self.metrics}
        results.update({"student_loss": loss_student, "dist_loss": loss_distillation})
        return results

# === [3. ä¸»ç¨‹å¼] ===
if __name__ == "__main__":
    print("ğŸš€ æº–å‚™é–‹å§‹çŸ¥è­˜è’¸é¤¾ (Knowledge Distillation)...")
    
    # 1. æª¢æŸ¥æ˜¯å¦æœ‰è€å¸«æ¨¡å‹
    if not os.path.exists(TEACHER_MODEL_PATH):
        print(f"âš ï¸  è­¦å‘Šï¼šæ‰¾ä¸åˆ°è€å¸«æ¨¡å‹ {TEACHER_MODEL_PATH}")
        print("è«‹ç­‰å¾…çµ„å“¡å‚³é€ 'teacher_resnet50_best.keras' å¾Œå†åŸ·è¡Œæ­¤ç¨‹å¼ã€‚")
        # é€™è£¡ç‚ºäº†ä¸å ±éŒ¯é€€å‡ºï¼Œæˆ‘å€‘å…ˆç”¨å‡çš„ Teacher ä»£æ›¿ (åƒ…ä¾›æ¸¬è©¦æµç¨‹)
        # print(">>> æ¸¬è©¦æ¨¡å¼ï¼šä½¿ç”¨æœªè¨“ç·´çš„ Teacher é€²è¡Œæ¨¡æ“¬ <<<")
        # teacher_model = tf.keras.applications.ResNet50V2(input_shape=(224,224,3), classes=2, weights=None, classifier_activation=None)
        exit() # æ­£å¼åŸ·è¡Œè«‹æŠŠé€™è¡Œç•™è‘—ï¼Œæ²’è€å¸«ä¸èƒ½è·‘
    else:
        print("âœ… è¼‰å…¥è€å¸«æ¨¡å‹...")
        teacher_model = models.load_model(TEACHER_MODEL_PATH)
        # å‡çµè€å¸«ï¼Œä¸è®“ä»–æ›´æ–° (ä»–å·²ç¶“å‡ºå¸«äº†)
        teacher_model.trainable = False 

    # 2. å»ºç«‹å…¨æ–°çš„å­¸ç”Ÿæ¨¡å‹
    student_model = build_student_model()
    
    # 3. å»ºç«‹è’¸é¤¾å™¨
    distiller = Distiller(student=student_model, teacher=teacher_model)
    
    distiller.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        metrics=['mae'],
        student_loss_fn=losses.MeanSquaredError(),
        distillation_loss_fn=losses.MeanSquaredError(),
        alpha=ALPHA
    )

    # 4. æº–å‚™è³‡æ–™
    print("ğŸ“¥ å»ºç«‹è³‡æ–™ç®¡ç·š...")
    train_ds = create_distillation_dataset()
    
    # è¨ˆç®—æ­¥æ•¸
    num_files = len(glob.glob(os.path.join(PROCESSED_DIR, '*', '*.npz')))
    steps_per_epoch = (num_files * 800) // BATCH_SIZE 

    # 5. é–‹å§‹è¨“ç·´
    print("ğŸ”¥ é–‹å§‹è’¸é¤¾è¨“ç·´ (Teacher -> Student)...")
    
    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(
        filepath='models/student_mobilenet_distilled.keras',
        save_best_only=True,
        monitor='mae', # ç›£æ§å­¸ç”Ÿæœ¬èº«çš„æº–ç¢ºåº¦
        mode='min'
    )

    distiller.fit(
        train_ds,
        epochs=EPOCHS,
        steps_per_epoch=steps_per_epoch,
        callbacks=[checkpoint_cb]
    )
    
    print("ğŸ‰ è’¸é¤¾å®Œæˆï¼å­¸ç”Ÿæ¨¡å‹å·²å„²å­˜ã€‚")