import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
import time

# ================= âš™ï¸ è¨­å®šå€ =================
MODEL_PATH = "models/litegaze_v2_distilled.tflite"  # TFLite æ¨¡å‹è·¯å¾‘
INPUT_SIZE = 60                                     # æ¨¡å‹è¼¸å…¥å¤§å° (60x60)
SMOOTHING_RATIO = 0.7                               # å¹³æ»‘ä¿‚æ•¸ (0~1)ï¼Œè¶Šé«˜è¶Šéˆæ•ï¼Œè¶Šä½è¶Šç©©
SENSITIVITY = 100                                   # è¦–ç·šç®­é ­é•·åº¦
# =============================================

class LiteGazeDemo:
    def __init__(self, model_path):
        # 1. åˆå§‹åŒ– TFLite è§£è­¯å™¨
        print(f"ğŸš€ Loading model from: {model_path}")
        try:
            self.interpreter = tf.lite.Interpreter(model_path=model_path)
            self.interpreter.allocate_tensors()
            
            self.input_details = self.interpreter.get_input_details()
            self.output_details = self.interpreter.get_output_details()
            
            # æª¢æŸ¥è¼¸å…¥å½¢ç‹€ (é æœŸ: [1, 60, 60, 1])
            input_shape = self.input_details[0]['shape']
            print(f"âœ… Model Input Shape: {input_shape}")
            
            # ç°¡å–®æª¢æŸ¥æ˜¯å¦ç‚ºå–®é€šé“ (é»‘ç™½)
            if input_shape[-1] != 1:
                print("âš ï¸ Warning: Model expects RGB input? Check your training script.")
            else:
                print("âœ… Model expects Grayscale input (Correct!)")
                
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            exit()

        # 2. åˆå§‹åŒ– MediaPipe Face Mesh (ç”¨ä¾†æŠ“è‡‰)
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        # å¹³æ»‘ç”¨çš„è®Šæ•¸
        self.prev_pitch = 0
        self.prev_yaw = 0

    def preprocess(self, face_img):
        """
        é—œéµæ­¥é©Ÿï¼šå°‡åœ–ç‰‡è½‰ç‚ºæ¨¡å‹çœ‹å¾—æ‡‚çš„æ ¼å¼
        1. Resize -> 60x60
        2. BGR -> Grayscale (é‡è¦!)
        3. Normalize -> 0~1
        4. Expand Dims -> (1, 60, 60, 1)
        """
        try:
            # Resize
            img = cv2.resize(face_img, (INPUT_SIZE, INPUT_SIZE))
            
            # ğŸ”¥ è½‰ç‚ºç°éš (é…åˆå­¸ç”Ÿæ¨¡å‹)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            # Normalize (0~1)
            img = img.astype(np.float32) / 255.0
            
            # å¢åŠ ç¶­åº¦: (60, 60) -> (1, 60, 60, 1)
            img = np.expand_dims(img, axis=0)
            img = np.expand_dims(img, axis=-1)
            
            return img
        except Exception as e:
            return None

    def predict(self, input_tensor):
        """ åŸ·è¡Œæ¨è«– """
        self.interpreter.set_tensor(self.input_details[0]['index'], input_tensor)
        self.interpreter.invoke()
        
        # å–å¾—è¼¸å‡º (å‡è¨­ index 0 æ˜¯ gaze vectorï¼Œå¦‚æœä¸æ˜¯è¦æª¢æŸ¥ output_details)
        # æˆ‘å€‘çš„æ¨¡å‹è¼¸å‡ºé †åºé€šå¸¸æ˜¯: [gaze_xy, pitch_logits, yaw_logits] æˆ–é¡ä¼¼
        # ä½†é€šå¸¸ä¸»è¦è¼¸å‡º (Gaze) æœƒåœ¨ç¬¬ä¸€å€‹
        gaze_vector = self.interpreter.get_tensor(self.output_details[0]['index'])
        return gaze_vector[0] # [pitch, yaw]

    def draw_gaze(self, frame, landmarks, pitch, yaw):
        """ ç•«å‡ºè¦–ç·šç®­é ­ """
        h, w, c = frame.shape
        
        # æ‰¾é¼»é ­ä½ç½® (Index 1 or 4) ä½œç‚ºèµ·é»
        nose_idx = 4
        nose_x = int(landmarks[nose_idx].x * w)
        nose_y = int(landmarks[nose_idx].y * h)
        
        # è¨ˆç®—çµ‚é» (å°‡ Pitch/Yaw è½‰æ›ç‚º 2D å‘é‡)
        # Pitch (ä¸Šä¸‹): è² å€¼å¾€ä¸Š
        # Yaw (å·¦å³): è² å€¼å¾€å³ (è¦–è§’ä¸åŒå¯èƒ½è¦åè½‰)
        
        dx = -np.sin(yaw) * SENSITIVITY
        dy = -np.sin(pitch) * SENSITIVITY
        
        end_x = int(nose_x + dx)
        end_y = int(nose_y + dy)
        
        # ç•«ç·š
        cv2.arrowedLine(frame, (nose_x, nose_y), (end_x, end_y), (0, 0, 255), 4)
        
        # é¡¯ç¤ºæ•¸å€¼
        cv2.putText(frame, f"Pitch: {pitch:.2f}", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(frame, f"Yaw:   {yaw:.2f}", (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

    def run(self):
        cap = cv2.VideoCapture(0) # é–‹å•Ÿ Webcam
        
        print("ğŸ“· Starting Webcam... Press 'q' to exit.")
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break
            
            # ç¿»è½‰é¡é ­ (åƒç…§é¡å­ä¸€æ¨£)
            frame = cv2.flip(frame, 1)
            h, w, _ = frame.shape
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # 1. åµæ¸¬äººè‡‰
            results = self.face_mesh.process(rgb_frame)
            
            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    # 2. è£åˆ‡äººè‡‰å€åŸŸ (ç°¡å–®ç‰ˆï¼šå– bounding box)
                    x_min, y_min = w, h
                    x_max, y_max = 0, 0
                    
                    for lm in face_landmarks.landmark:
                        x, y = int(lm.x * w), int(lm.y * h)
                        if x < x_min: x_min = x
                        if x > x_max: x_max = x
                        if y < y_min: y_min = y
                        if y > y_max: y_max = y
                    
                    # ç¨å¾®æ“´å¤§ä¸€é»ç¯„åœï¼ŒåŒ…å«æ•´å€‹é ­
                    margin_x = int((x_max - x_min) * 0.2)
                    margin_y = int((y_max - y_min) * 0.2)
                    x_min = max(0, x_min - margin_x)
                    x_max = min(w, x_max + margin_x)
                    y_min = max(0, y_min - margin_y)
                    y_max = min(h, y_max + margin_y)
                    
                    face_img = frame[y_min:y_max, x_min:x_max]
                    
                    if face_img.size == 0: continue

                    # ç•«å‡ºäººè‡‰æ¡†
                    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (255, 255, 0), 2)
                    
                    # 3. é è™•ç†
                    input_tensor = self.preprocess(face_img)
                    
                    if input_tensor is not None:
                        # 4. æ¨è«–
                        pred = self.predict(input_tensor)
                        pitch, yaw = pred[0], pred[1]
                        
                        # 5. å¹³æ»‘è™•ç† (Exponential Moving Average)
                        pitch = SMOOTHING_RATIO * pitch + (1 - SMOOTHING_RATIO) * self.prev_pitch
                        yaw = SMOOTHING_RATIO * yaw + (1 - SMOOTHING_RATIO) * self.prev_yaw
                        
                        self.prev_pitch = pitch
                        self.prev_yaw = yaw
                        
                        # 6. ç•«å‡ºè¦–ç·š
                        self.draw_gaze(frame, face_landmarks.landmark, pitch, yaw)

            cv2.imshow('LiteGaze Final Demo', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
                
        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    # è«‹ç¢ºä¿æª”åæ­£ç¢º
    demo = LiteGazeDemo(MODEL_PATH)
    demo.run()