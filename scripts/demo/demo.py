import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
import time
import sys
import traceback  # ğŸ”¥ æ–°å¢ï¼šç”¨æ–¼å°å‡ºè©³ç´°éŒ¯èª¤å †ç–Š

# === è¨­å®šå€ ===
TFLITE_MODEL_PATH = 'models/litegaze_student.tflite'
INPUT_SIZE = (60, 60)
SMOOTH_WINDOW = 5

# ç©©å®šåŒ–åƒæ•¸
history_pitch = []
history_yaw = []

def moving_average(new_val, history):
    history.append(new_val)
    if len(history) > SMOOTH_WINDOW:
        history.pop(0)
    return sum(history) / len(history)

# ç”¨æ–¼åœ¨ç•«é¢ä¸Šå°å­—çš„è¼”åŠ©å‡½å¼
def draw_debug_text(img, text, line_num, color=(0, 255, 0)):
    cv2.putText(img, text, (10, 30 + line_num * 25), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

try:
    # --- Step 1: æ¨¡å‹è¼‰å…¥ ---
    print("\n[Step 1] æ­£åœ¨è¼‰å…¥ TFLite æ¨¡å‹...")
    if not tf.io.gfile.exists(TFLITE_MODEL_PATH):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ: {TFLITE_MODEL_PATH}")
        
    interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    # ğŸ”¥ å°å‡ºæ¨¡å‹è³‡è¨Šï¼Œå¹«åŠ©é™¤éŒ¯
    print(f"   ğŸ‘‰ æ¨¡å‹è¼¸å…¥å½¢ç‹€: {input_details[0]['shape']}")
    print(f"   ğŸ‘‰ æ¨¡å‹è¼¸å…¥é¡å‹: {input_details[0]['dtype']}")
    print("âœ… TFLite æ¨¡å‹è¼‰å…¥å®Œæˆ")

    # --- Step 2: MediaPipe åˆå§‹åŒ– ---
    print("[Step 2] æ­£åœ¨åˆå§‹åŒ– MediaPipe...")
    mp_face_mesh = mp.solutions.face_mesh
    face_mesh = mp_face_mesh.FaceMesh(
        refine_landmarks=True,
        max_num_faces=1,
        min_detection_confidence=0.6,
        min_tracking_confidence=0.6
    )
    print("âœ… MediaPipe åˆå§‹åŒ–å®Œæˆ")

    # çœ¼ç›é—œéµé»ç´¢å¼•
    LEFT_EYE = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]
    RIGHT_EYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]

    # --- Step 3: é–‹å•Ÿæ”å½±æ©Ÿ ---
    print("[Step 3] æ­£åœ¨é–‹å•Ÿæ”å½±æ©Ÿ...")
    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
    if not cap.isOpened():
        print("âš ï¸ ç„¡æ³•æ‰“é–‹ Camera 0ï¼Œå˜—è©¦ Camera 1...")
        cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)

    if not cap.isOpened():
        raise RuntimeError("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ä»»ä½•æ”å½±æ©Ÿï¼è«‹æª¢æŸ¥è£ç½®é€£ç·šã€‚")

    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    print("ğŸš€ LiteGaze å•Ÿå‹•æˆåŠŸï¼")

    fps_time = time.time()
    frame_count = 0
    fail_count = 0  # è¨ˆç®—é€£çºŒå¤±æ•—æ¬¡æ•¸

    while True:
        try:
            success, frame = cap.read()
            if not success:
                fail_count += 1
                print(f"âš ï¸ ç„¡æ³•è®€å–å½±åƒ ({fail_count}/10)")
                if fail_count > 10:
                    raise RuntimeError("âŒ æ”å½±æ©Ÿè¨Šè™Ÿä¸­æ–·ï¼Œç¨‹å¼å¼·åˆ¶çµæŸã€‚")
                continue
            
            fail_count = 0 # é‡ç½®å¤±æ•—è¨ˆæ•¸
            
            # FPS è¨ˆç®—
            frame_count += 1
            fps = 0
            if time.time() - fps_time > 1.0:
                fps = frame_count
                frame_count = 0
                fps_time = time.time()

            # å½±åƒå‰è™•ç†
            frame = cv2.flip(frame, 1)
            h, w, _ = frame.shape
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # MediaPipe æ¨è«–
            results = face_mesh.process(rgb_frame)

            # ç•«é¢å„€è¡¨æ¿ (HUD)
            draw_debug_text(frame, f"FPS: {fps}", 0, (0, 255, 255))

            if results.multi_face_landmarks:
                draw_debug_text(frame, "Face: Detected", 1, (0, 255, 0))
                
                for face_landmarks in results.multi_face_landmarks:
                    pts = np.array([np.multiply([p.x, p.y], [w, h]).astype(int) for p in face_landmarks.landmark])
                    
                    eye_centers = []
                    gaze_results = []

                    for i, eye_idxs in enumerate([LEFT_EYE, RIGHT_EYE]):
                        eye_pts = pts[eye_idxs]
                        
                        # ğŸ”¥ å®‰å…¨é‚Šç•Œæª¢æŸ¥ï¼šé˜²æ­¢è£åˆ‡è¶…å‡ºç•«é¢
                        x_min, y_min = np.min(eye_pts, axis=0)
                        x_max, y_max = np.max(eye_pts, axis=0)
                        
                        # æ“´å¤§ä¸€é»ç¯„åœï¼Œä½†é™åˆ¶åœ¨ 0~w, 0~h ä¹‹é–“
                        x1 = max(0, x_min - 5)
                        y1 = max(0, y_min - 5)
                        x2 = min(w, x_max + 5)
                        y2 = min(h, y_max + 5)

                        # ç¹ªè£½çœ¼ç›æ¡†æ¡† (é™¤éŒ¯ç”¨)
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 255, 0), 1)

                        eye_img = frame[y1:y2, x1:x2]
                        
                        if eye_img.size > 0 and eye_img.shape[0] > 5 and eye_img.shape[1] > 5:
                            # æ¨¡å‹æ¨è«–
                            eye_input = cv2.resize(cv2.cvtColor(eye_img, cv2.COLOR_BGR2RGB), INPUT_SIZE)
                            eye_input = (eye_input.astype(np.float32) / 255.0)[np.newaxis, :]
                            
                            interpreter.set_tensor(input_details[0]['index'], eye_input)
                            interpreter.invoke()
                            gaze = interpreter.get_tensor(output_details[0]['index'])[0]
                            
                            eye_centers.append(((x1+x2)//2, (y1+y2)//2))
                            gaze_results.append(gaze)
                        else:
                            print(f"âš ï¸ è·³ééå°çš„çœ¼ç›å€åŸŸ: {eye_img.shape}")

                    if gaze_results:
                        avg_pitch = np.mean([g[0] for g in gaze_results])
                        avg_yaw = np.mean([g[1] for g in gaze_results])
                        
                        smooth_p = moving_average(avg_pitch, history_pitch)
                        smooth_y = moving_average(avg_yaw, history_yaw)

                        # é¡¯ç¤ºæ•¸å€¼
                        draw_debug_text(frame, f"Pitch: {smooth_p:.2f}", 2)
                        draw_debug_text(frame, f"Yaw:   {smooth_y:.2f}", 3)

                        # ç¹ªè£½è¦–ç·šç®­é ­
                        for center in eye_centers:
                            dx = -150 * np.sin(smooth_y)
                            dy = -150 * np.sin(smooth_p)
                            end_pt = (int(center[0] + dx), int(center[1] + dy))
                            cv2.arrowedLine(frame, center, end_pt, (0, 0, 255), 2, tipLength=0.3)
            else:
                draw_debug_text(frame, "Face: Searching...", 1, (0, 0, 255))

            cv2.imshow('LiteGaze Enhanced Debug', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("ğŸ‘‰ ä½¿ç”¨è€…æŒ‰ä¸‹ 'q' é›¢é–‹")
                break
        
        except KeyboardInterrupt:
            print("\nğŸ›‘ å¼·åˆ¶ä¸­æ–· (KeyboardInterrupt)")
            break
        except Exception as e:
            # ğŸ”¥ é€™æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼šæ•æ‰è¿´åœˆå…§çš„ä»»ä½•éŒ¯èª¤ä¸¦å°å‡º
            print("\nâŒ åŸ·è¡ŒæœŸé–“ç™¼ç”ŸéŒ¯èª¤ï¼")
            print("==========================================")
            traceback.print_exc()
            print("==========================================")
            break

except Exception as e:
    # æ•æ‰åˆå§‹åŒ–éšæ®µçš„éŒ¯èª¤
    print("\nâŒ åˆå§‹åŒ–å¤±æ•—ï¼")
    print("==========================================")
    traceback.print_exc()
    print("==========================================")

finally:
    # ç¢ºä¿è³‡æºé‡‹æ”¾ (å³ä½¿å ±éŒ¯ä¹ŸæœƒåŸ·è¡Œ)
    print("\n[Cleanup] æ­£åœ¨é‡‹æ”¾è³‡æº...")
    if 'cap' in locals() and cap.isOpened():
        cap.release()
    cv2.destroyAllWindows()
    print("ğŸ‘‹ ç¨‹å¼å·²å®‰å…¨çµæŸ")