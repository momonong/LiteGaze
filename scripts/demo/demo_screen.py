import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
from screeninfo import get_monitors

# ================= âš™ï¸ è¨­å®šå€ =================
MODEL_PATH = "models/litegaze_v2_distilled.tflite"
INPUT_SIZE = 60
SMOOTHING_RATIO = 0.85   # ç¨å¾®èª¿é«˜ä¸€é»é»ï¼Œè®“ç´…é»æ›´ç©©
GAZE_SENSITIVITY = 1200  # è¦–ç·šéˆæ•åº¦
# ============================================

class LiteGazeScreenDemo:
    def __init__(self, model_path):
        # 1. è¢å¹•è¨­å®š
        try:
            monitor = get_monitors()[0]
            self.screen_w = monitor.width
            self.screen_h = monitor.height
        except:
            self.screen_w = 1920
            self.screen_h = 1080
        self.screen_cx = self.screen_w // 2
        self.screen_cy = self.screen_h // 2

        # 2. æ¨¡å‹è¼‰å…¥
        print(f"ğŸš€ Loading model from: {model_path}")
        try:
            self.interpreter = tf.lite.Interpreter(model_path=model_path)
            self.interpreter.allocate_tensors()
            self.input_details = self.interpreter.get_input_details()
            self.output_details = self.interpreter.get_output_details()
        except Exception as e:
            print(f"âŒ Error: {e}")
            exit()

        # 3. Face Mesh
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True, # ğŸ”¥ é–‹å•Ÿ Refine landmarks ä»¥ç²å¾—æ›´æº–çš„è™¹è†œé»
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        self.smooth_pitch = 0
        self.smooth_yaw = 0

        # ğŸ”¥ å®šç¾©çœ¼ç›çš„ç‰¹å¾µé»ç´¢å¼• (MediaPipe æ¨™æº–)
        self.LEFT_EYE_IDX = [33, 133, 160, 159, 158, 144, 145, 153]
        self.RIGHT_EYE_IDX = [362, 263, 387, 386, 385, 373, 374, 380]

    def preprocess(self, face_img):
        try:
            img = cv2.resize(face_img, (INPUT_SIZE, INPUT_SIZE))
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # è½‰é»‘ç™½
            img = img.astype(np.float32) / 255.0
            img = np.expand_dims(img, axis=0)
            img = np.expand_dims(img, axis=-1)
            return img
        except:
            return None

    def predict(self, input_tensor):
        self.interpreter.set_tensor(self.input_details[0]['index'], input_tensor)
        self.interpreter.invoke()
        return self.interpreter.get_tensor(self.output_details[0]['index'])[0]

    def map_gaze_to_screen(self, pitch, yaw):
        dx = -yaw * GAZE_SENSITIVITY
        dy = -pitch * GAZE_SENSITIVITY
        return int(np.clip(self.screen_cx + dx, 0, self.screen_w)), int(np.clip(self.screen_cy + dy, 0, self.screen_h))

    def draw_eye_boxes(self, frame, landmarks, w, h):
        """ ğŸ”¥ æ–°å¢åŠŸèƒ½ï¼šç•«å‡ºçœ¼ç›çš„æ¡†æ¡† """
        for eye_idx, color in [(self.LEFT_EYE_IDX, (0, 255, 255)), (self.RIGHT_EYE_IDX, (0, 255, 255))]:
            # å–å¾—è©²çœ¼ç›æ‰€æœ‰é»çš„åº§æ¨™
            eye_points = []
            for idx in eye_idx:
                lm = landmarks[idx]
                eye_points.append([int(lm.x * w), int(lm.y * h)])
            
            eye_points = np.array(eye_points)
            
            # è¨ˆç®—å¤–æ¥çŸ©å½¢ (Bounding Box)
            x, y, ew, eh = cv2.boundingRect(eye_points)
            
            # ç¨å¾®å¤–æ“´ä¸€é»é»ï¼Œæ¯”è¼ƒå¥½çœ‹
            margin = 5
            x = max(0, x - margin)
            y = max(0, y - margin)
            ew += margin * 2
            eh += margin * 2
            
            # ç•«æ¡† (é»ƒè‰²)
            cv2.rectangle(frame, (x, y), (x + ew, y + eh), color, 2)

    def run(self):
        cap = cv2.VideoCapture(0)
        cv2.namedWindow('LiteGaze', cv2.WND_PROP_FULLSCREEN)
        cv2.setWindowProperty('LiteGaze', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break
            
            # æº–å‚™ç•«å¸ƒ
            canvas = np.zeros((self.screen_h, self.screen_w, 3), dtype=np.uint8)
            cv2.line(canvas, (self.screen_cx, 0), (self.screen_cx, self.screen_h), (0, 100, 0), 1)
            cv2.line(canvas, (0, self.screen_cy), (self.screen_w, self.screen_cy), (0, 100, 0), 1)

            # è™•ç†å½±åƒ
            frame_flipped = cv2.flip(frame, 1)
            h, w, _ = frame_flipped.shape
            rgb_frame = cv2.cvtColor(frame_flipped, cv2.COLOR_BGR2RGB)
            results = self.face_mesh.process(rgb_frame)
            
            detected = False
            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    # 1. å–å¾—äººè‡‰ ROI (çµ¦æ¨¡å‹ç”¨)
                    x_coords = [lm.x for lm in face_landmarks.landmark]
                    y_coords = [lm.y for lm in face_landmarks.landmark]
                    x_min, x_max = int(min(x_coords)*w), int(max(x_coords)*w)
                    y_min, y_max = int(min(y_coords)*h), int(max(y_coords)*h)
                    
                    # æ“´å¤§äººè‡‰æ¡†
                    margin_x, margin_y = int((x_max-x_min)*0.25), int((y_max-y_min)*0.35)
                    face_roi = frame_flipped[
                        max(0, y_min-margin_y):min(h, y_max+margin_y),
                        max(0, x_min-margin_x):min(w, x_max+margin_x)
                    ]
                    
                    if face_roi.size > 0:
                        input_tensor = self.preprocess(face_roi)
                        if input_tensor is not None:
                            # 2. æ¨è«–èˆ‡å¹³æ»‘
                            prediction = self.predict(input_tensor)
                            if len(prediction) >= 2:
                                pitch, yaw = prediction[0], prediction[1]
                            else:
                                continue
                            self.smooth_pitch = SMOOTHING_RATIO * self.smooth_pitch + (1 - SMOOTHING_RATIO) * pitch
                            self.smooth_yaw = SMOOTHING_RATIO * self.smooth_yaw + (1 - SMOOTHING_RATIO) * yaw
                            
                            # 3. è¦–è¦ºåŒ–
                            gx, gy = self.map_gaze_to_screen(self.smooth_pitch, self.smooth_yaw)
                            
                            # ç•«ç´…é» (è¦–ç·š)
                            cv2.circle(canvas, (gx, gy), 25, (0, 0, 255), -1)
                            # ç•«å…‰æšˆ (è®“ç´…é»çœ‹èµ·ä¾†åƒé›·å°„)
                            cv2.circle(canvas, (gx, gy), 40, (0, 0, 255), 2)
                            
                            # ğŸ”¥ 4. ç•«æ¡†æ¡†ï¼šäººè‡‰ (ç¶ è‰²) + çœ¼ç› (é»ƒè‰²)
                            # ç•«äººè‡‰æ¡†
                            cv2.rectangle(frame_flipped, 
                                        (max(0, x_min-margin_x), max(0, y_min-margin_y)), 
                                        (min(w, x_max+margin_x), min(h, y_max+margin_y)), 
                                        (0, 255, 0), 2)
                            
                            # ç•«çœ¼ç›æ¡† (æ–°å¢çš„å‡½å¼)
                            self.draw_eye_boxes(frame_flipped, face_landmarks.landmark, w, h)
                            
                            detected = True

            # é¡¯ç¤ºå·¦ä¸‹è§’å°ç•«é¢
            cam_small = cv2.resize(frame_flipped, (320, 240))
            canvas[self.screen_h-240:self.screen_h, 0:320] = cam_small
            
            # åŠ å…¥æ–‡å­—è³‡è¨Š
            status = f"Pitch: {self.smooth_pitch:.2f} Yaw: {self.smooth_yaw:.2f}"
            cv2.putText(canvas, status, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 1)

            cv2.imshow('LiteGaze', canvas)
            if cv2.waitKey(1) & 0xFF == ord('q'): break
                
        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    demo = LiteGazeScreenDemo(MODEL_PATH)
    demo.run()