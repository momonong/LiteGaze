import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
import time

# === âš™ï¸ è¨­å®šå€ ===
MODEL_PATH = 'models/litegaze_v2_win.tflite' 
INPUT_SIZE = (60, 60)

# è¦–ç·šç¹ªè£½é•·åº¦
AXIS_LENGTH = 200 

def draw_gaze(image_in, pitchyaw, thickness=2, color=(0, 0, 255)):
    """ç•«å‡ºè¦–ç·šå‘é‡"""
    image_out = image_in
    (h, w) = image_in.shape[:2]
    length = AXIS_LENGTH
    
    # æ¨¡å‹çš„è¼¸å‡ºæ˜¯ Radiansï¼Œæˆ‘å€‘è½‰æˆå‘é‡
    pitch, yaw = pitchyaw[0], pitchyaw[1]
    
    # æ•¸å­¸è½‰æ› (Spherical to Cartesian)
    # æ³¨æ„ï¼šé€™è£¡çš„åæ¨™ç³»å¯èƒ½éœ€è¦æ ¹æ“šæ¨¡å‹è¨“ç·´æ™‚çš„å®šç¾©å¾®èª¿
    # å‡è¨­ï¼šXå‘å³, Yå‘ä¸‹, Zå‘å¾Œ (OpenCV Standard)
    dx = -length * np.sin(yaw) * np.cos(pitch)
    dy = -length * np.sin(pitch)
    
    center = (w // 2, h // 2)
    end_point = (int(center[0] + dx), int(center[1] + dy))
    
    cv2.arrowedLine(image_out, center, end_point, color, thickness, tipLength=0.2)
    return image_out

def main():
    # 1. è¼‰å…¥ TFLite æ¨¡å‹
    print("â³ Loading TFLite model...")
    interpreter = tf.lite.Interpreter(model_path=MODEL_PATH)
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    # æ‰¾åˆ°è¦–ç·šè¼¸å‡ºçš„ Index (é€šå¸¸æ˜¯ç¬¬ä¸€å€‹ï¼Œä½†ä¹Ÿå¯èƒ½æ˜¯åˆ†é¡çš„ Logits)
    # æˆ‘å€‘åœ¨ export æ™‚ output é †åºæ˜¯ [gaze, pitch_logits, yaw_logits]
    output_index = output_details[0]['index'] 

    # 2. åˆå§‹åŒ– Face Mesh (ç”¨ä¾†æŠ“è‡‰)
    mp_face_mesh = mp.solutions.face_mesh
    face_mesh = mp_face_mesh.FaceMesh(
        max_num_faces=1,
        refine_landmarks=True,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    )

    cap = cv2.VideoCapture(0)
    
    print("ğŸš€ Starting Demo... Press 'q' to quit.")
    
    while cap.isOpened():
        success, image = cap.read()
        if not success:
            break

        # ç¿»è½‰åœ–ç‰‡ (åƒç…§é¡å­ä¸€æ¨£)
        image = cv2.flip(image, 1)
        h, w, _ = image.shape
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # åµæ¸¬äººè‡‰
        results = face_mesh.process(rgb_image)

        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                # ç°¡å–®å–è‡‰éƒ¨é‚Šç•Œ (Bounding Box)
                x_min, y_min = w, h
                x_max, y_max = 0, 0
                
                for lm in face_landmarks.landmark:
                    x, y = int(lm.x * w), int(lm.y * h)
                    if x < x_min: x_min = x
                    if y < y_min: y_min = y
                    if x > x_max: x_max = x
                    if y > y_max: y_max = y
                
                # ç¨å¾®æ“´å¤§æ¡†æ¡† (Padding) ä»¥åŒ…å«æ•´å¼µè‡‰
                pad_x = int((x_max - x_min) * 0.1)
                pad_y = int((y_max - y_min) * 0.1)
                x_min = max(0, x_min - pad_x)
                y_min = max(0, y_min - pad_y)
                x_max = min(w, x_max + pad_x)
                y_max = min(h, y_max + pad_y)

                # è£åˆ‡è‡‰éƒ¨
                face_img = image[y_min:y_max, x_min:x_max]
                
                if face_img.size == 0: continue

                # === æ ¸å¿ƒï¼šå‰è™•ç† & æ¨è«– ===
                try:
                    # Resize to 60x60
                    input_img = cv2.resize(face_img, INPUT_SIZE)
                    
                    # Normalize (0~1 æˆ– 0~255 å–æ±ºæ–¼è¨“ç·´æ•¸æ“š)
                    # å‡è¨­è¨“ç·´æ™‚æ˜¯ float 0-1 (å› ç‚ºç”¨äº† tf.image.convert_image_dtype æˆ– clip 0-1)
                    input_data = input_img.astype(np.float32) / 255.0
                    input_data = np.expand_dims(input_data, axis=0) # Add Batch dim

                    # æ¨è«–
                    interpreter.set_tensor(input_details[0]['index'], input_data)
                    interpreter.invoke()
                    
                    # å–å¾—çµæœ (Pitch, Yaw)
                    pred_gaze = interpreter.get_tensor(output_index)[0]
                    
                    # ç¹ªè£½çµæœ
                    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
                    
                    # åœ¨è‡‰éƒ¨ä¸­å¿ƒç•«ç®­é ­
                    # æˆ‘å€‘æŠŠç®­é ­ç•«åœ¨è‡‰çš„æ¡†æ¡†ä¸Šï¼Œæ¯”è¼ƒæ¸…æ¥š
                    center_x = (x_min + x_max) // 2
                    center_y = (y_min + y_max) // 2
                    
                    # é¡¯ç¤ºæ•¸å€¼
                    text = f"P: {pred_gaze[0]:.2f}, Y: {pred_gaze[1]:.2f}"
                    cv2.putText(image, text, (x_min, y_min - 10), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

                    # ç‚ºäº†è¦–è¦ºåŒ–ï¼Œæˆ‘å€‘å‰µä¸€å€‹å°è¦–çª—å°ˆé–€ç•«ç®­é ­ï¼Œæˆ–è€…ç›´æ¥ç•«åœ¨è‡‰ä¸Š
                    # é€™è£¡ç°¡å–®ç•«åœ¨è‡‰ä¸Š
                    length = 100
                    pitch, yaw = pred_gaze[0], pred_gaze[1]
                    dx = -length * np.sin(yaw)
                    dy = -length * np.sin(pitch)
                    cv2.arrowedLine(image, (center_x, center_y), 
                                  (int(center_x + dx), int(center_y + dy)), 
                                  (0, 0, 255), 4)
                                  
                except Exception as e:
                    print(f"Inference Error: {e}")

        cv2.imshow('LiteGaze V2 Demo', image)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()