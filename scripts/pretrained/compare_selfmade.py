import torch
import torch.nn as nn
from torchvision import transforms, models
import os
import glob
from PIL import Image
import numpy as np
from tqdm import tqdm

# ================= âš™ï¸ è¨­å®š =================
DATA_DIR = 'data/selfmade_combined'  # ä½ çš„ 9000 å¼µåœ–
TEACHER_PATH = 'models/L2CSNet_gaze360.pkl'
STUDENT_PATH = 'models/student_mobilenet_3people_9k.pth'
DEVICE = torch.device('cuda') # è©•ä¼°æ™‚ç”¨ GPU è·‘æ¯”è¼ƒå¿«
BATCH_SIZE = 128
# ==========================================

# æ¨¡å‹å®šç¾© (çœç•¥é‡è¤‡éƒ¨åˆ†ï¼Œä¿æŒä¸€è‡´)
class L2CS_ResNet50(nn.Module):
    def __init__(self, num_bins=90):
        super(L2CS_ResNet50, self).__init__()
        self.model = models.resnet50(weights=None)
        self.model.fc = nn.Linear(2048, num_bins * 2)
    def forward(self, x):
        x = self.model(x)
        return x[:, :90], x[:, 90:]

class L2CS_MobileNetV3(nn.Module):
    def __init__(self, num_bins=90):
        super(L2CS_MobileNetV3, self).__init__()
        self.backbone = models.mobilenet_v3_large(weights=None)
        in_features = self.backbone.classifier[3].in_features
        self.backbone.classifier[3] = nn.Linear(in_features, num_bins * 2)
    def forward(self, x):
        x = self.backbone(x)
        return x[:, :90], x[:, 90:]

def compute_gaze(logits):
    softmax = nn.Softmax(dim=1)
    prob = softmax(logits)
    idx = torch.arange(90, dtype=torch.float32).to(logits.device)
    gaze = torch.sum(prob * idx, dim=1) * 4 - 180
    return gaze

def get_model_size(model):
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    size_all_mb = (param_size + buffer_size) / 1024**2
    return size_all_mb

def main():
    print("ğŸš€ é–‹å§‹è©•ä¼°æ¨¡å‹æŒ‡æ¨™...")
    
    # 1. è¼‰å…¥æ¨¡å‹
    teacher = L2CS_ResNet50().to(DEVICE)
    ckpt = torch.load(TEACHER_PATH, map_location=DEVICE)
    # (è¼‰å…¥ Teacher æ¬Šé‡ä»£ç¢¼çœç•¥ï¼ŒåŒå‰)
    state = {}
    for k, v in ckpt.items():
        if 'fc_pitch' in k or 'fc_yaw' in k: continue
        nk = 'model.'+k if not k.startswith('model.') else k
        state[nk] = v
    if 'fc_pitch_gaze.weight' in ckpt:
        state['model.fc.weight'] = torch.cat((ckpt['fc_pitch_gaze.weight'], ckpt['fc_yaw_gaze.weight']), 0)
        state['model.fc.bias'] = torch.cat((ckpt['fc_pitch_gaze.bias'], ckpt['fc_yaw_gaze.bias']), 0)
    teacher.load_state_dict(state, strict=False)
    teacher.eval()
    
    student = L2CS_MobileNetV3().to(DEVICE)
    student.load_state_dict(torch.load(STUDENT_PATH, map_location=DEVICE))
    student.eval()

    # 2. è¨ˆç®—æ¨¡å‹å¤§å°
    t_size = get_model_size(teacher)
    s_size = get_model_size(student)
    print(f"\nğŸ“Š æ¨¡å‹å¤§å°æ¯”è¼ƒ:")
    print(f"Teacher (ResNet50)   : {t_size:.2f} MB")
    print(f"Student (MobileNetV3): {s_size:.2f} MB")
    print(f"ğŸ‘‰ å£“ç¸®ç‡: {t_size/s_size:.1f}x (ç¸®å°äº† {t_size/s_size:.1f} å€)")

    # 3. æº–å‚™è³‡æ–™
    files = glob.glob(os.path.join(DATA_DIR, "*.jpg"))
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # 4. é–‹å§‹è¨ˆç®—èª¤å·®
    pitch_errors = []
    yaw_errors = []
    
    print(f"\nrunning evaluation on {len(files)} images...")
    
    # ç‚ºäº†æ–¹ä¾¿ï¼Œæˆ‘å€‘æ‰‹å‹•åš batch
    batch_imgs = []
    for i, f in enumerate(tqdm(files)):
        try:
            img = transform(Image.open(f).convert('RGB'))
            batch_imgs.append(img)
        except: continue
        
        if len(batch_imgs) == BATCH_SIZE or i == len(files)-1:
            if not batch_imgs: break
            
            inp = torch.stack(batch_imgs).to(DEVICE)
            
            with torch.no_grad():
                tp, ty = teacher(inp)
                sp, sy = student(inp)
                
                t_pitch = compute_gaze(tp)
                t_yaw = compute_gaze(ty)
                s_pitch = compute_gaze(sp)
                s_yaw = compute_gaze(sy)
                
                # è¨ˆç®—çµ•å°èª¤å·® (Absolute Error)
                p_err = torch.abs(t_pitch - s_pitch)
                y_err = torch.abs(t_yaw - s_yaw)
                
                pitch_errors.extend(p_err.cpu().numpy())
                yaw_errors.extend(y_err.cpu().numpy())
            
            batch_imgs = []

    # 5. ç¸½çµå ±å‘Š
    mae_pitch = np.mean(pitch_errors)
    mae_yaw = np.mean(yaw_errors)
    
    print("\n" + "="*40)
    print("ğŸ“ˆ æº–ç¢ºåº¦è©•ä¼°å ±å‘Š (Accuracy Report)")
    print("="*40)
    print(f"MAE Pitch : {mae_pitch:.2f}Â°")
    print(f"MAE Yaw   : {mae_yaw:.2f}Â°")
    print(f"Avg Error : {(mae_pitch + mae_yaw)/2:.2f}Â°")
    print("="*40)
    print("ğŸ’¡ è§£è®€ï¼š")
    print("- < 3.0Â° : å®Œç¾è’¸é¤¾ (Perfect)")
    print("- 3.0Â°~5.0Â° : å„ªè‰¯ (Good)")
    print("- > 5.0Â° : å°šå¯ (Acceptable)")

if __name__ == "__main__":
    main()