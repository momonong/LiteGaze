import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
import glob
import os
import cv2
import numpy as np
from PIL import Image

# ================= âš™ï¸ è¨­å®šå€ =================
# è«‹æŒ‡å‘ä½ åŸæœ¬é‚£å€‹æœ€å¤§çš„ MPII è³‡æ–™é›†
DATA_DIR = 'data/distill_images' 
TEACHER_PATH = 'models/L2CSNet_gaze360.pkl'
STUDENT_SAVE_PATH = 'models/student_mobilenet_mpii_logits.pth'

DEVICE = torch.device('cuda')
BATCH_SIZE = 64      # 5090 å¯ä»¥é–‹å¤§ä¸€é»ï¼Œè·‘å¿«ä¸€é»
EPOCHS = 20          # MPII è³‡æ–™å¤šï¼Œ20 è¼ªå°±å¾ˆå¼·äº†
LR = 1e-4            # æ¨™æº–å­¸ç¿’ç‡
TEMP = 5.0           # è’¸é¤¾æº«åº¦ (è®“åˆ†ä½ˆæ›´å¹³æ»‘ï¼Œæ›´å¥½å­¸)
# ============================================

# === 1. æ¨¡å‹å®šç¾© ===
class L2CS_ResNet50(nn.Module):
    def __init__(self, num_bins=90):
        super(L2CS_ResNet50, self).__init__()
        self.model = models.resnet50(weights=None)
        self.model.fc = nn.Linear(2048, num_bins * 2)
    def forward(self, x):
        x = self.model(x)
        return x[:, :90], x[:, 90:]

class L2CS_MobileNetV3(nn.Module):
    def __init__(self, num_bins=90):
        super(L2CS_MobileNetV3, self).__init__()
        # è¼‰å…¥ ImageNet é è¨“ç·´ï¼Œç¢ºä¿éª¨å¹¹æœ‰åŸºç¤è¦–è¦ºèƒ½åŠ›
        self.backbone = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)
        in_features = self.backbone.classifier[3].in_features
        self.backbone.classifier[3] = nn.Linear(in_features, num_bins * 2)
    def forward(self, x):
        x = self.backbone(x)
        return x[:, :90], x[:, 90:]

# === 2. è³‡æ–™é›† (ç›´æ¥è®€å–åˆ‡å¥½çš„åœ–) ===
class MPIIDataset(Dataset):
    def __init__(self, root):
        # æ”¯æ´ jpg å’Œ png
        self.files = glob.glob(os.path.join(root, "*.jpg")) + glob.glob(os.path.join(root, "*.png"))
        print(f"ğŸ“Š è¼‰å…¥ MPII è³‡æ–™é›†: å…± {len(self.files)} å¼µåœ–ç‰‡")
        
        # è¨“ç·´æ™‚åŠ å…¥ä¸€é»é»å¢å¼·ï¼Œè®“æ¨¡å‹æ›´å¼·å£¯
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def __len__(self): return len(self.files)
    
    def __getitem__(self, i):
        try:
            img = Image.open(self.files[i]).convert('RGB')
            return self.transform(img)
        except:
            return torch.zeros(3, 224, 224)

# === 3. è’¸é¤¾ Loss (KL Divergence) ===
def distillation_loss(student_logits, teacher_logits, T):
    # Logits -> Softmax åˆ†ä½ˆ
    soft_targets = nn.functional.softmax(teacher_logits / T, dim=1)
    soft_prob = nn.functional.log_softmax(student_logits / T, dim=1)
    # è¨ˆç®—åˆ†ä½ˆå·®ç•°
    loss = nn.KLDivLoss(reduction='batchmean')(soft_prob, soft_targets) * (T**2)
    return loss

def main():
    print(f"ğŸš€ å•Ÿå‹• MPII Logit Distillation on {DEVICE}...")
    
    # A. æº–å‚™è€å¸«
    print("ğŸ‘¨â€ğŸ« Loading Teacher...")
    teacher = L2CS_ResNet50().to(DEVICE)
    ckpt = torch.load(TEACHER_PATH, map_location=DEVICE)
    state = {}
    for k, v in ckpt.items():
        if 'fc_pitch' in k or 'fc_yaw' in k: continue
        nk = 'model.'+k if not k.startswith('model.') else k
        state[nk] = v
    if 'fc_pitch_gaze.weight' in ckpt:
        state['model.fc.weight'] = torch.cat((ckpt['fc_pitch_gaze.weight'], ckpt['fc_yaw_gaze.weight']), 0)
        state['model.fc.bias'] = torch.cat((ckpt['fc_pitch_gaze.bias'], ckpt['fc_yaw_gaze.bias']), 0)
    teacher.load_state_dict(state, strict=False)
    teacher.eval() # è€å¸«ä¸è¨“ç·´
    
    # B. æº–å‚™å­¸ç”Ÿ
    print("ğŸ‘¶ Initializing Student (MobileNetV3)...")
    student = L2CS_MobileNetV3().to(DEVICE)
    student.train()
    
    # C. æº–å‚™è³‡æ–™
    dataset = MPIIDataset(DATA_DIR)
    if len(dataset) == 0:
        print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è³‡æ–™ï¼è«‹ç¢ºèª datasets/distill_images æ˜¯å¦å­˜åœ¨ã€‚")
        return

    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    optimizer = optim.Adam(student.parameters(), lr=LR)
    
    print(f"ğŸ”¥ é–‹å§‹è¨“ç·´ (Temp={TEMP})...")
    
    for epoch in range(EPOCHS):
        total_loss = 0
        batch_count = 0
        
        for i, images in enumerate(dataloader):
            images = images.to(DEVICE)
            
            # 1. è€å¸«çœ‹åœ– -> ç”¢ç”Ÿ Logits (ä¸åªæ˜¯ä¸€å€‹è§’åº¦ï¼Œè€Œæ˜¯90å€‹ä¿¡å¿ƒåˆ†æ•¸)
            with torch.no_grad():
                t_pitch_logits, t_yaw_logits = teacher(images)
            
            # 2. å­¸ç”Ÿçœ‹åœ– -> ç”¢ç”Ÿ Logits
            s_pitch_logits, s_yaw_logits = student(images)
            
            # 3. è¨ˆç®— KL Loss
            loss_p = distillation_loss(s_pitch_logits, t_pitch_logits, TEMP)
            loss_y = distillation_loss(s_yaw_logits, t_yaw_logits, TEMP)
            loss = loss_p + loss_y
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            batch_count += 1
            
            if i % 20 == 0:
                print(f"Epoch {epoch+1} | Batch {i}/{len(dataloader)} | Loss: {loss.item():.4f}", end='\r')
                
        avg_loss = total_loss / batch_count
        print(f"\nâœ… Epoch {epoch+1} Done. Avg Distill Loss: {avg_loss:.4f}")
        
        # æ¯ 5 è¼ªå­˜ä¸€æ¬¡ï¼Œä»¥é˜²è¬ä¸€
        if (epoch+1) % 5 == 0:
            torch.save(student.state_dict(), STUDENT_SAVE_PATH)

    print(f"ğŸ‰ è¨“ç·´å®Œæˆï¼æ¨¡å‹å·²å­˜ç‚º: {STUDENT_SAVE_PATH}")
    print("ğŸ‘‰ é€™å€‹æ¨¡å‹æ“æœ‰ MPII çš„å¤§æ•¸æ“šçŸ¥è­˜ï¼Œä»¥åŠè€å¸«çš„ Logit åˆ¤æ–·é‚è¼¯ã€‚")

if __name__ == '__main__':
    main()