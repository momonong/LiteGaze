import torch
import torch.nn as nn
from l2cs import Pipeline
from torchvision import transforms
import cv2
import numpy as np

# ç¹¼æ‰¿ä¸¦ä¿®æ”¹å®˜æ–¹ Pipelineï¼Œåªç‚ºäº†æŒ–å‡º Raw Logits
class TeacherPipeline(Pipeline):
    def step_logits(self, frame):
        # é€™æ˜¯å®˜æ–¹ step() çš„ç°¡åŒ–ç‰ˆï¼Œé‡é»æ˜¯å›å‚³ raw_logits
        face_imgs = self._get_face_crops(frame) # ä½¿ç”¨å®˜æ–¹å…§å»ºçš„è£åˆ‡é‚è¼¯
        if face_imgs is None or len(face_imgs) == 0:
            return None, None
        
        # è½‰æˆ Tensor
        face_imgs = torch.stack(face_imgs).to(self.device)
        
        with torch.no_grad():
            # ğŸ”¥ é—œéµï¼šç›´æ¥å‘¼å« modelï¼Œå–å¾— [Batch, 90] çš„ logits
            pitch_logits, yaw_logits = self.model(face_imgs)
            
        return pitch_logits, yaw_logits

    def _get_face_crops(self, frame):
        # å·ç”¨å®˜æ–¹çš„ç§æœ‰æ–¹æ³•ä¾†åšä¸€æ¨£çš„è£åˆ‡
        results = self.detect_faces(frame)
        if results.bboxes is None or len(results.bboxes) == 0:
            return []
        
        face_imgs = []
        for bbox in results.bboxes:
            bbox = bbox.astype(int)
            x_min, y_min, x_max, y_max = bbox[0], bbox[1], bbox[2], bbox[3]
            # Padding é‚è¼¯éœ€èˆ‡ Teacher ä¸€è‡´
            h, w, _ = frame.shape
            # (å®˜æ–¹å¯èƒ½æœ‰è‡ªå·±çš„ paddingï¼Œé€™è£¡æˆ‘å€‘ç›¡é‡æ¨¡ä»¿æˆ–ç›´æ¥ç”¨ detector è£åˆ‡)
            # ç‚ºäº†ç°¡å–®ï¼Œæˆ‘å€‘é€™è£¡ç›´æ¥åˆ‡ bbox (å®˜æ–¹ pipeline å…§éƒ¨æœ‰åšè™•ç†)
            face_img = frame[y_min:y_max, x_min:x_max]
            
            # é è™•ç†
            if face_img.size == 0: continue
            img = cv2.resize(face_img, (224, 224))
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = transforms.ToTensor()(img)
            img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)
            face_imgs.append(img)
            
        return face_imgs