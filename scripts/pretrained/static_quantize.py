import torch
import torch.nn as nn
from torchvision import models, transforms
import os
import glob
from PIL import Image
import numpy as np
from tqdm import tqdm

# ================= âš™ï¸ è¨­å®š =================
DATA_DIR = 'data/selfmade_combined'
STUDENT_PATH = 'models/student_mobilenet_3people_9k.pth'
QUANTIZED_PATH = 'models/student_mobilenet_static_int8.pth'
CALIBRATE_BATCH = 100 
DEVICE = torch.device('cpu') # é‡åŒ–é©—è­‰ä¸€å®šè¦ç”¨ CPU
# ==========================================

# ğŸ”¥ é—œéµä¿®æ”¹ï¼šä½¿ç”¨ quantization ç‰ˆæœ¬çš„ MobileNetV3
# é€™å€‹ç‰ˆæœ¬æŠŠæ‰€æœ‰çš„ '+' æ›æˆäº† FloatFunctionalï¼Œè§£æ±ºäº†å ±éŒ¯å•é¡Œ
from torchvision.models.quantization import mobilenet_v3_large, MobileNet_V3_Large_QuantizedWeights

class L2CS_MobileNetV3_Quant(nn.Module):
    def __init__(self, num_bins=90):
        super(L2CS_MobileNetV3_Quant, self).__init__()
        # ä½¿ç”¨æ”¯æ´é‡åŒ–çš„éª¨å¹¹ç¶²çµ¡ (quantize=False ä»£è¡¨å…ˆä»¥ FP32 æ¨¡å¼è¼‰å…¥ï¼Œæº–å‚™é€²è¡Œ PTQ)
        self.backbone = mobilenet_v3_large(weights=None, quantize=False)
        
        # ä¿®æ”¹æœ€å¾Œä¸€å±¤
        in_features = self.backbone.classifier[3].in_features
        self.backbone.classifier[3] = nn.Linear(in_features, num_bins * 2)
        
        # Stub (é‡åŒ–é‚Šç•Œæ¨™è¨˜)
        self.quant = torch.ao.quantization.QuantStub()
        self.dequant = torch.ao.quantization.DeQuantStub()

    def forward(self, x):
        x = self.quant(x)
        x = self.backbone(x)
        x = self.dequant(x)
        return x[:, :90], x[:, 90:]

def compute_gaze(logits):
    softmax = nn.Softmax(dim=1)
    prob = softmax(logits)
    idx = torch.arange(90, dtype=torch.float32).to(logits.device)
    gaze = torch.sum(prob * idx, dim=1) * 4 - 180
    return gaze.item()

def main():
    print(f"ğŸ“¥ è¼‰å…¥ FP32 æ¨¡å‹: {STUDENT_PATH}")
    
    # 1. æº–å‚™æ¨¡å‹
    # æ³¨æ„ï¼šæˆ‘å€‘ç¾åœ¨ç”¨çš„æ˜¯ Quantizable çš„éª¨å¹¹ï¼Œçµæ§‹è·ŸåŸæœ¬çš„ç•¥æœ‰ä¸åŒ
    # ä½†æ¬Šé‡å¤§éƒ¨åˆ†æ˜¯å…¼å®¹çš„ï¼Œæˆ‘å€‘å¯ä»¥é€é strict=False ç¡¬åƒé€²å»
    model = L2CS_MobileNetV3_Quant()
    state_dict = torch.load(STUDENT_PATH, map_location='cpu')
    
    # é€™è£¡å¯èƒ½æœƒæœ‰ä¸€äº› key ä¸åŒ¹é… (å› ç‚º quantizable model çµæ§‹è®Šäº†)
    # æ²’é—œä¿‚ï¼Œåªè¦ backbone.features çš„æ¬Šé‡æœ‰é€²å»å°±å¥½
    missing, unexpected = model.load_state_dict(state_dict, strict=False)
    if len(missing) > 0:
        print(f"âš ï¸ éƒ¨åˆ†æ¬Šé‡æœªè¼‰å…¥ (é€™æ˜¯æ­£å¸¸çš„ï¼Œå› ç‚ºæ›äº† Quantizable éª¨å¹¹): {len(missing)} keys")
    
    model.eval()

    # 2. è¨­å®šé‡åŒ–é…ç½® (ä½¿ç”¨ onednn)
    backend = 'onednn' # ğŸ”¥ ä½ æŒ‡å®šçš„å¾Œç«¯
    print(f"âš™ï¸ ä½¿ç”¨å¾Œç«¯: {backend}")
    
    model.qconfig = torch.ao.quantization.get_default_qconfig(backend)
    torch.backends.quantized.engine = backend
    
    # 3. èåˆç®—å­ (Fusion) - é€™æ˜¯åŠ é€Ÿçš„é—œéµ
    # MobileNetV3 çš„æ¨™æº–èåˆï¼š Conv+BN+ReLU
    print("ğŸ”¥ æ­£åœ¨èåˆç®—å­ (Fuse Modules)...")
    model.backbone.fuse_model(is_qat=False)

    # 4. æº–å‚™é‡åŒ–
    print("ğŸ‘€ æº–å‚™é‡åŒ– (Prepare)...")
    torch.ao.quantization.prepare(model, inplace=True)
    
    # 5. æ ¡æº– (Calibration)
    print("ğŸ“ æ­£åœ¨æ ¡æº– (Calibration) - è®€å– 100 å¼µåœ–ç‰‡...")
    files = glob.glob(os.path.join(DATA_DIR, "*.jpg"))[:CALIBRATE_BATCH]
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    with torch.no_grad():
        for f in tqdm(files):
            try:
                img = transform(Image.open(f).convert('RGB')).unsqueeze(0)
                model(img)
            except: continue
            
    # 6. è½‰æ› (Convert)
    print("ğŸ”„ æ­£åœ¨è½‰æ›ç‚º INT8 (Convert)...")
    torch.ao.quantization.convert(model, inplace=True)
    
    # 7. å­˜æª”èˆ‡å¤§å°æ¯”è¼ƒ
    torch.save(model.state_dict(), QUANTIZED_PATH)
    size_fp32 = os.path.getsize(STUDENT_PATH) / 1024**2
    size_int8 = os.path.getsize(QUANTIZED_PATH) / 1024**2
    
    print("\n" + "="*30)
    print(f"ğŸ“Š æ¨¡å‹ç˜¦èº«æˆæœ:")
    print(f"FP32: {size_fp32:.2f} MB")
    print(f"INT8: {size_int8:.2f} MB")
    print(f"ğŸ‘‰ å£“ç¸®ç‡: {size_fp32/size_int8:.1f}x")
    print("="*30)

    # 8. é©—è­‰ç²¾åº¦
    print("âš–ï¸ æ­£åœ¨è©•ä¼° INT8 æ¨¡å‹ç²¾åº¦ (å‰ 200 å¼µ)...")
    pitch_diffs = []
    yaw_diffs = []
    
    # è¼‰å…¥ä¸€å€‹åŸå§‹ FP32 æ¨¡å‹åšå°ç…§
    # é€™è£¡æˆ‘å€‘ç”¨å›åŸæœ¬çš„ classï¼Œç¢ºä¿å°ç…§çµ„æ˜¯æ­£ç¢ºçš„
    from torchvision import models as original_models
    class L2CS_MobileNetV3_Original(nn.Module):
        def __init__(self, num_bins=90):
            super().__init__()
            self.backbone = original_models.mobilenet_v3_large(weights=None)
            in_features = self.backbone.classifier[3].in_features
            self.backbone.classifier[3] = nn.Linear(in_features, num_bins * 2)
        def forward(self, x):
            x = self.backbone(x)
            return x[:, :90], x[:, 90:]
            
    fp32_model = L2CS_MobileNetV3_Original()
    fp32_model.load_state_dict(torch.load(STUDENT_PATH, map_location='cpu'))
    fp32_model.eval()
    
    eval_files = glob.glob(os.path.join(DATA_DIR, "*.jpg"))[:200]
    
    with torch.no_grad():
        for f in tqdm(eval_files):
            try:
                img = transform(Image.open(f).convert('RGB')).unsqueeze(0)
                
                # FP32
                p1, y1 = fp32_model(img)
                deg_p1 = compute_gaze(p1)
                deg_y1 = compute_gaze(y1)
                
                # INT8
                p2, y2 = model(img)
                deg_p2 = compute_gaze(p2)
                deg_y2 = compute_gaze(y2)
                
                pitch_diffs.append(abs(deg_p1 - deg_p2))
                yaw_diffs.append(abs(deg_y1 - deg_y2))
            except: continue

    mae_pitch = np.mean(pitch_diffs)
    mae_yaw = np.mean(yaw_diffs)
    
    print("\n" + "="*40)
    print("ğŸ“‰ é‡åŒ–èª¤å·®å ±å‘Š (Quantization Loss Report)")
    print("="*40)
    print(f"Pitch MAE Loss: {mae_pitch:.4f}Â°")
    print(f"Yaw   MAE Loss: {mae_yaw:.4f}Â°")
    print("="*40)

if __name__ == "__main__":
    main()