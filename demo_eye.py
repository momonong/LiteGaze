import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
import time
import traceback

# === è¨­å®šå€ ===
TFLITE_MODEL_PATH = 'models/litegaze_student.tflite'
INPUT_SIZE = (60, 60)
SMOOTH_WINDOW = 5

# ğŸ”¥ è¦–ç·šæ˜ å°„åƒæ•¸ (é€™æ˜¯æ§åˆ¶ç´…çƒæ€éº¼è·‘çš„é—œéµ)
X_SENSITIVITY = 1000   # æ°´å¹³éˆæ•åº¦ (è¶Šå¤§è·‘è¶Šå¿«)
Y_SENSITIVITY = 1200   # å‚ç›´éˆæ•åº¦

# ğŸ”¥ æ ¡æ­£åç§» (å¦‚æœä½ çœ‹æ­£ä¸­é–“æ™‚ç´…çƒä¸åœ¨ä¸­é–“ï¼Œæ”¹é€™è£¡)
# è² å€¼ä»£è¡¨ç´…çƒæœƒå¾€ä¸Š/å·¦ä¿®ï¼Œæ­£å€¼å¾€ä¸‹/å³ä¿®
OFFSET_PITCH = -0.15 
OFFSET_YAW = 0.0

# ç©©å®šåŒ–æ­·å²ç´€éŒ„
history_pitch = []
history_yaw = []

def moving_average(new_val, history):
    history.append(new_val)
    if len(history) > SMOOTH_WINDOW:
        history.pop(0)
    return sum(history) / len(history)

def draw_debug_text(img, text, line_num, color=(0, 255, 0)):
    cv2.putText(img, text, (10, 30 + line_num * 25), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

def main():
    cap = None
    try:
        # --- Step 1: æ¨¡å‹è¼‰å…¥ ---
        print("[Step 1] æ­£åœ¨è¼‰å…¥ TFLite æ¨¡å‹...")
        interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)
        interpreter.allocate_tensors()
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        print("âœ… TFLite æ¨¡å‹è¼‰å…¥å®Œæˆ")

        # --- Step 2: MediaPipe åˆå§‹åŒ– ---
        print("[Step 2] æ­£åœ¨åˆå§‹åŒ– MediaPipe...")
        mp_face_mesh = mp.solutions.face_mesh
        face_mesh = mp_face_mesh.FaceMesh(
            refine_landmarks=True, max_num_faces=1,
            min_detection_confidence=0.6, min_tracking_confidence=0.6
        )

        LEFT_EYE = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]
        RIGHT_EYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]

        # --- Step 3: é–‹å•Ÿæ”å½±æ©Ÿ ---
        print("[Step 3] æ­£åœ¨é–‹å•Ÿæ”å½±æ©Ÿ...")
        cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
        if not cap.isOpened():
            cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)
        
        # è¨­å®šè§£æåº¦
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        # å–å¾—å¯¦éš›ç•«é¢å¤§å° (ç”¨æ–¼æ˜ å°„åº§æ¨™)
        frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        print("ğŸš€ LiteGaze Gaze Tracking å•Ÿå‹•ï¼(æŒ‰ 'q' é›¢é–‹)")

        while True:
            # é€™è£¡ä¸éœ€è¦é¡å¤–çš„ tryï¼Œå› ç‚ºå¤–å±¤å·²ç¶“æœ‰äº†ï¼Œé™¤éä½ æƒ³æ•æ‰å–®ä¸€å¹€çš„éŒ¯èª¤
            success, frame = cap.read()
            if not success:
                print("âš ï¸ æ‰å¹€ä¸­...")
                continue

            # é¡åƒç¿»è½‰ (è®“æ“ä½œæ¯”è¼ƒç›´è¦º)
            frame = cv2.flip(frame, 1)
            h, w, _ = frame.shape
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # MediaPipe æ¨è«–
            results = face_mesh.process(rgb_frame)

            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    pts = np.array([np.multiply([p.x, p.y], [w, h]).astype(int) for p in face_landmarks.landmark])
                    
                    eye_centers = []
                    gaze_results = []

                    # è£åˆ‡çœ¼ç›ä¸¦æ¨è«–
                    for eye_idxs in [LEFT_EYE, RIGHT_EYE]:
                        eye_pts = pts[eye_idxs]
                        x_min, y_min = np.min(eye_pts, axis=0)
                        x_max, y_max = np.max(eye_pts, axis=0)
                        
                        # å®‰å…¨é‚Šç•Œ
                        x1, y1 = max(0, x_min-5), max(0, y_min-5)
                        x2, y2 = min(w, x_max+5), min(h, y_max+5)

                        eye_img = frame[y1:y2, x1:x2]
                        
                        # æª¢æŸ¥çœ¼ç›åœ–ç‰‡æ˜¯å¦æœ‰æ•ˆ
                        if eye_img.size > 0 and eye_img.shape[0] > 5 and eye_img.shape[1] > 5:
                            eye_input = cv2.resize(cv2.cvtColor(eye_img, cv2.COLOR_BGR2RGB), INPUT_SIZE)
                            eye_input = (eye_input.astype(np.float32) / 255.0)[np.newaxis, :]
                            
                            interpreter.set_tensor(input_details[0]['index'], eye_input)
                            interpreter.invoke()
                            gaze = interpreter.get_tensor(output_details[0]['index'])[0]
                            
                            eye_centers.append(((x1+x2)//2, (y1+y2)//2))
                            gaze_results.append(gaze)

                    if gaze_results:
                        # 1. è¨ˆç®—å¹³å‡è§’åº¦
                        avg_pitch = np.mean([g[0] for g in gaze_results])
                        avg_yaw = np.mean([g[1] for g in gaze_results])
                        
                        # 2. å¹³æ»‘åŒ–
                        smooth_p = moving_average(avg_pitch, history_pitch)
                        smooth_y = moving_average(avg_yaw, history_yaw)

                        # 3. ğŸ”¥ è¦–ç·šæ˜ å°„æ ¸å¿ƒé‚è¼¯ ğŸ”¥
                        # å…¬å¼: åç§»é‡ = tan(è§’åº¦ - æ ¡æ­£å€¼) * éˆæ•åº¦
                        delta_x = np.tan(smooth_y - OFFSET_YAW) * X_SENSITIVITY
                        delta_y = np.tan(smooth_p - OFFSET_PITCH) * Y_SENSITIVITY
                        
                        # ç®—å‡ºè¢å¹•åº§æ¨™ (å‡è¨­ç•«é¢ä¸­å¿ƒ = è¢å¹•ä¸­å¿ƒ)
                        gaze_x = int(frame_w / 2 + delta_x)
                        gaze_y = int(frame_h / 2 + delta_y) # Pitch è² å€¼å¾€ä¸Šï¼Œä½†åœ¨å½±åƒåº§æ¨™ Y å¾€ä¸Šæ˜¯è®Šå°ï¼Œé€™è£¡ç›´æ¥åŠ å³å¯ (è¦–æ¨¡å‹å®šç¾©è€Œå®š)

                        # ç¹ªè£½ç´…çƒ (ä»£è¡¨è¦–ç·šè½é»)
                        cv2.circle(frame, (gaze_x, gaze_y), 15, (0, 0, 255), -1)
                        # ç•«ä¸€æ¢ç·šé€£åˆ°çœ¼ç› (è¦–è¦ºè¼”åŠ©)
                        for center in eye_centers:
                             cv2.line(frame, center, (gaze_x, gaze_y), (0, 255, 255), 1)

                        # é¡¯ç¤ºæ•¸å€¼
                        draw_debug_text(frame, f"Pitch: {smooth_p:.2f}", 0)
                        draw_debug_text(frame, f"Yaw:   {smooth_y:.2f}", 1)
                        draw_debug_text(frame, f"Gaze: ({gaze_x}, {gaze_y})", 2, (0, 255, 255))

            cv2.imshow('LiteGaze - Eye Tracking', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    except Exception as e:
        print("âŒ ç™¼ç”ŸéŒ¯èª¤:")
        traceback.print_exc()
    finally:
        print("[Cleanup] é‡‹æ”¾è³‡æº...")
        if cap and cap.isOpened():
            cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main()